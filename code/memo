정규화 

1. 과소적합, 과대적합
1)과소적합
 -> 문제가 어려워서 문제자체를 제대로 풀지 못하는것
  -> 해결방법 : 학습을 더많이 돌린다, 데이터를 늘린다, 모델용량을 키운다
  
2)과대적합
 -> 전체 모집단에서 일부데이터를 가져와 학습 하므로 가져온 일부 데이터에만 맞에 학습하였기 때문에 새로운 데이터에대해서는 잘 풀지 못함.(암기한것만 풀줄암)
  -> 해결방법 : 양질의 데이터를 확보한다.(비용이 많이 든다), 문제를 어렵게 만들어 외우기 힘들게 만든다(정규화)
  
  
3) 정규화

L2,L1,드롭아웃,잡음 주입, 배치 정규화

# 1) L2_Loss 
-> 절댓값이 큰 파라미터에 대해 불이익을 주어 값의 폭주를 막는다. 기왕이면 작은 절댓값의 파라미터들로 문제를 풀도록 압박하는 정규화 기법
-> L2 페널티값을 손실 함수에 더해준다. // 각각의 파라미터 wi가 수정될 때 기존의 손실 기울기에 L2 decay * wi의 값을 더해서 처리하면 된다.
-> 각 미니배치의 학습 때마다 고정세율의 세금처럼 뗴어내서 바쳐야 함을 의미한다. 이런 부담은 파라미터들의 절댓값을 줄이는 방향으로 압박하여 지나치게 큰 값으로의 폭주를 막고 빈익빈 부익부 현상을 억제한다.


# 2) L1_Loss
-> L1_Loss도 L1 페널티값을 손실 함수에 더해주는 정규화 기법이다.
-> 파라미터 wi가 수정될 때 기존의 손실 기울기에 α sign(wi)값을 더해서 처리하면 된다.
-> L2 손실처럼 자기 값에서 일정한 비율을 덜어내는 것이 아니라 일률적으로 정해진 값을 덜어내는 것, 작은값을 가진 파라미터에는 치명적이다.

-> **가중치 분포에 미치는 영향이 크다. 무슨 이유에서든 가중치들이 0 혹은 0에 가까운 원소를 많이 포함하는 희소한 텐서가 되는것이 바람직하면 L1손실을 사용하면 된다.**


# 3) 드롭아웃

-> 입력 또는 어떤 계층의 출력을 다음 계층에서 모두 이용하지 않고 일부만 이용하여 규제시키는 방법이다. 간단하면서도 매우 효과적으로, 요즘 널리 이용됨
-> 예를 들어 다층 퍼셉트론 신경망에 드롭아웃 처리하는 그림을 보면 노드들이 없어져서 계산량이 줄어든것 같지만. 순전파 역전파 처리에서의 마스킹 처리를 수행해야 하기 때문에 계산량은 늘어난다.
-> 40%확률로 값을 0 으로 만든다고 하였을때 나머지 60%를 증폭시킨다. 100/60 = 1.67배 이렇게 하면 계층 출력 전체 평균 기댓값이 드롭아웃 계층 적용 전과 같은 수준으로 유지된다.
-> 이때 곱했던 배율을 손실기울기에도 똑같이 전달해야하므로 마스크 정보를 역전파용 보조 정보로서 챙겨야한다.

# 4) 잡음 주입
-> 두 계층 사이에 잡음 주입 계층을 삽입하여 이 계층이 아래 계층의 출력에 적당한 형태의 잡음을 추가하여 위 계층에 전달하는 정규화 기법이다.


# 5) 배치 정규화

-> 미니배치 내의 데이터들에 대해 벡터 성분별로 정규화를 수행하는 방식이다.
(평균 0 , 편차 1)
-> 정규화는 입력 성분 간의 분포 차이로 인한 가중치 학습의 불균형을 방지하기 위해 도입되었다.
-> [m,n]형태의 미니배치 데이터가 있을때 크기가 n인 벡터 성분 각각이 독립으로 보고 벡터 성분별로 그룹 n개를 만들어 각 그룹 안에 있는 데이터 값 m개를 정규화 한다.
정규화 과정을 거치고 나면 벡터 각 성분의 데이터들은 모두 평균0, 표준편차 1의 분포를 가지게 되며 전체 데이터셋이 아닌 미니배치 내에서 불균형이 사라지게 된다.

하지만 불균형을 획일적으로 맞춰 오히려 바람직하지 못한 결과를 가져올 가능성도 있다. 그에 따라 다시 scale factor라는 파라미터값을 곱하고, shift factor라는 파라미터를 더한다.

--> 너무 작은 미니배치보다는 크기를 조금키우면 더 좋아진다. 작은 미니배치에서는 너무 극단적으로 나타날수 있기 때문이다. 처리횟수가 줄어들어 학습 속도를 높이는 부수적인 효과도 얻게 되기 때문에 앞으로 더욱 널리 이용될 것이다.

1. 미니배치 평균 계산
2. 미니배치 분산 계산
3. 데이터 정규화 처리 수행
4. 크기 요소와 이동 요소에 의한 선형 연산 수행
